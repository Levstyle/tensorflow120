{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TFRECORDS = './raw_data/train.tfrecords'\n",
    "TEST_TFRECORDS = './raw_data/test.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_vals(sess):\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = sess.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print('Variable: ', k, 'Shape: ', v.shape, 'Value: ', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def get_loss_acc(self, train_images, train_labels, reuse_variables=None):\n",
    "        with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):\n",
    "            y_ = self.inference(train_images)\n",
    "            y = tf.to_float(train_labels)\n",
    "            y.set_shape([128, 10])\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    labels=y, logits=y_))\n",
    "            correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return loss, accuracy\n",
    "\n",
    "\n",
    "    def get_weight_variable(self, shape):\n",
    "        weights = tf.get_variable(\"weights\", shape,\n",
    "                                  initializer=tf.truncated_normal_initializer(\n",
    "                                          stddev=0.1))\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def inference(self, train_images):\n",
    "        with tf.variable_scope('layer1'):\n",
    "            w = self.get_weight_variable([784, 10])\n",
    "            b = tf.get_variable(\"biases\", [10],\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "            y = tf.matmul(train_images, w) + b\n",
    "        return y\n",
    "\n",
    "\n",
    "    def train(self, file_names, global_step):\n",
    "        with tf.variable_scope(tf.get_variable_scope()):\n",
    "            samples = input_fn(file_names, 128)\n",
    "            train_images, train_labels = samples\n",
    "            loss, acc = self.get_loss_acc(train_images, train_labels)\n",
    "            opt = tf.train.GradientDescentOptimizer(0.5)\n",
    "            train_op = opt.minimize(loss, global_step=global_step)\n",
    "            return train_op, loss, acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parse_fn(batch_size):\n",
    "    def read_examples(examples):\n",
    "        features = {}\n",
    "        features['label'] = tf.FixedLenFeature([], tf.int64)\n",
    "        features['image_raw'] = tf.FixedLenFeature([], tf.string)\n",
    "        features = tf.parse_example(examples, features)\n",
    "        images = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "        images.set_shape([batch_size, 784])\n",
    "        images = tf.cast(images, tf.float32) * (1. / 255) - 0.5\n",
    "        labels = features['label']\n",
    "        one_hot_labels = tf.to_float(tf.one_hot(labels, 10, 1, 0))\n",
    "        return images, one_hot_labels\n",
    "\n",
    "    return read_examples\n",
    "\n",
    "\n",
    "def input_fn(file_names, batch_size, epoch=None):\n",
    "    _parse_fn = generate_parse_fn(batch_size)\n",
    "    files = tf.data.Dataset.list_files(file_names)\n",
    "    # number_of_cpu is the value of worker.vcore in xml file\n",
    "    dataset = files.apply(tf.contrib.data.parallel_interleave(\n",
    "            tf.data.TFRecordDataset,\n",
    "            cycle_length=4 * 2))\n",
    "    # prefetch will buffer the previos op and improve the performance\n",
    "    dataset = dataset.prefetch(buffer_size=batch_size)\n",
    "    # times: user defined\n",
    "    dataset = dataset.shuffle(buffer_size=batch_size * 4)\n",
    "    # buffer the shuffle op and improve the perfromance\n",
    "    dataset = dataset.prefetch(buffer_size=batch_size)\n",
    "    dataset = dataset.repeat(epoch)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(_parse_fn, num_parallel_calls=4)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-c12cb5efe6ef>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "model_a = Model()\n",
    "train_op, loss, acc = model_a.train(TRAIN_TFRECORDS, global_step)\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "model_b = Model()\n",
    "_, valid_loss, valid_acc = model_a.train(TEST_TFRECORDS,global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'global_step:0' shape=() dtype=int64_ref>, <tf.Variable 'layer1/weights:0' shape=(784, 10) dtype=float32_ref>, <tf.Variable 'layer1/biases:0' shape=(10,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "print(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:  layer1/weights:0 Shape:  (784, 10) Value:  [[-0.02052911  0.11269705  0.08209549 ...,  0.09361597  0.08152776\n",
      "  -0.04943234]\n",
      " [ 0.13435555  0.0461245   0.04200288 ...,  0.03626484  0.01160972\n",
      "  -0.0092969 ]\n",
      " [ 0.04860592 -0.03488415  0.07515283 ..., -0.02267688 -0.01706903\n",
      "  -0.00920649]\n",
      " ..., \n",
      " [ 0.07202993  0.01894343 -0.06945126 ...,  0.02244743 -0.07327471\n",
      "  -0.07766617]\n",
      " [-0.10550149 -0.01919532  0.07699977 ..., -0.15903479 -0.12641491\n",
      "   0.04359466]\n",
      " [-0.00423116  0.09942098 -0.0248875  ..., -0.03972405 -0.03542226\n",
      "   0.06878631]]\n",
      "Variable:  layer1/biases:0 Shape:  (10,) Value:  [ 0.02276097 -0.06405236  0.03249261  0.02673912 -0.0010454   0.01680639\n",
      "  0.00035756 -0.01868777  0.03820047 -0.05357032]\n",
      "........................................training done........................................\n",
      "Variable:  layer1/weights:0 Shape:  (784, 10) Value:  [[-0.02052911  0.11269705  0.08209549 ...,  0.09361597  0.08152776\n",
      "  -0.04943234]\n",
      " [ 0.13435555  0.0461245   0.04200288 ...,  0.03626484  0.01160972\n",
      "  -0.0092969 ]\n",
      " [ 0.04860592 -0.03488415  0.07515283 ..., -0.02267688 -0.01706903\n",
      "  -0.00920649]\n",
      " ..., \n",
      " [ 0.07202993  0.01894343 -0.06945126 ...,  0.02244743 -0.07327471\n",
      "  -0.07766617]\n",
      " [-0.10550149 -0.01919532  0.07699977 ..., -0.15903479 -0.12641491\n",
      "   0.04359466]\n",
      " [-0.00423116  0.09942098 -0.0248875  ..., -0.03972405 -0.03542226\n",
      "   0.06878631]]\n",
      "Variable:  layer1/biases:0 Shape:  (10,) Value:  [ 0.02276097 -0.06405236  0.03249261  0.02673912 -0.0010454   0.01680639\n",
      "  0.00035756 -0.01868777  0.03820047 -0.05357032]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for __ in range(100):\n",
    "    _, loss_val, acc_val, step = sess.run([train_op, loss, acc, global_step])\n",
    "#     print('loss: ', loss_val, ' acc_val: ', acc_val, ' step: ', step)\n",
    "print_trainable_vals(sess)\n",
    "print('........................................training done........................................')\n",
    "for __ in range(100):\n",
    "    loss_val, acc_val, step = sess.run([valid_loss, valid_acc, global_step])\n",
    "#     print('loss: ', loss_val, ' acc_val: ', acc_val, ' step: ', step) \n",
    "\n",
    "print_trainable_vals(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'global_step:0' shape=() dtype=int64_ref>, <tf.Variable 'layer1/weights:0' shape=(784, 10) dtype=float32_ref>, <tf.Variable 'layer1/biases:0' shape=(10,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "print(tf.global_variables())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
